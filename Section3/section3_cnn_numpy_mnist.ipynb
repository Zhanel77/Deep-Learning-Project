{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "489dde36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: CNN from scratch (NumPy core)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e15b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (60000, 1, 28, 28) (60000,) Test: (10000, 1, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Dataset: MNIST via OpenML (NumPy arrays)\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "\n",
    "X = mnist.data.astype(np.float32) / 255.0  # (70000, 784)\n",
    "y = mnist.target.astype(np.int64)          # labels 0..9\n",
    "\n",
    "# Reshape to NCHW for CNN: (N, C, H, W)\n",
    "X = X.reshape(-1, 1, 28, 28)\n",
    "\n",
    "# Standard split: 60k train / 10k test\n",
    "X_train, y_train = X[:60000], y[:60000]\n",
    "X_test,  y_test  = X[60000:], y[60000:]\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape, \"Test:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea66fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, num_classes=10):\n",
    "    # y: (N,)\n",
    "    oh = np.zeros((len(y), num_classes), dtype=np.float32)\n",
    "    oh[np.arange(len(y)), y] = 1.0\n",
    "    return oh\n",
    "\n",
    "def softmax(logits):\n",
    "    # logits: (N, C)\n",
    "    z = logits - np.max(logits, axis=1, keepdims=True)\n",
    "    exp = np.exp(z)\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "def accuracy(logits, y_true):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return np.mean(preds == y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b6dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 CNN layers (NumPy): im2col/col2im Conv + Pool + FC\n",
    "def im2col(x, kH, kW, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Convert input (N, C, H, W) into columns for fast convolution.\n",
    "    \"\"\"\n",
    "    N, C, H, W = x.shape\n",
    "    H_out = (H + 2*pad - kH) // stride + 1\n",
    "    W_out = (W + 2*pad - kW) // stride + 1\n",
    "\n",
    "    x_p = np.pad(x, ((0,0),(0,0),(pad,pad),(pad,pad)), mode=\"constant\")\n",
    "\n",
    "    cols = np.zeros((N, C, kH, kW, H_out, W_out), dtype=x.dtype)\n",
    "    for i in range(kH):\n",
    "        i_end = i + stride * H_out\n",
    "        for j in range(kW):\n",
    "            j_end = j + stride * W_out\n",
    "            cols[:, :, i, j, :, :] = x_p[:, :, i:i_end:stride, j:j_end:stride]\n",
    "\n",
    "    cols = cols.transpose(0,4,5,1,2,3).reshape(N*H_out*W_out, -1)\n",
    "    return cols, H_out, W_out\n",
    "\n",
    "def col2im(cols, x_shape, kH, kW, stride=1, pad=0, H_out=None, W_out=None):\n",
    "    \"\"\"\n",
    "    Reverse of im2col. Used to build dX for convolution.\n",
    "    \"\"\"\n",
    "    N, C, H, W = x_shape\n",
    "    H_out = H_out if H_out is not None else (H + 2*pad - kH)//stride + 1\n",
    "    W_out = W_out if W_out is not None else (W + 2*pad - kW)//stride + 1\n",
    "\n",
    "    cols = cols.reshape(N, H_out, W_out, C, kH, kW).transpose(0,3,4,5,1,2)\n",
    "    x_p = np.zeros((N, C, H + 2*pad, W + 2*pad), dtype=cols.dtype)\n",
    "\n",
    "    for i in range(kH):\n",
    "        i_end = i + stride * H_out\n",
    "        for j in range(kW):\n",
    "            j_end = j + stride * W_out\n",
    "            x_p[:, :, i:i_end:stride, j:j_end:stride] += cols[:, :, i, j, :, :]\n",
    "\n",
    "    return x_p[:, :, pad:pad+H, pad:pad+W] if pad > 0 else x_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08af96c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    \"\"\"\n",
    "    2D convolution layer (NCHW) implemented with im2col.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, kH, kW, stride=1, pad=0):\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.kH, self.kW = kH, kW\n",
    "        self.stride, self.pad = stride, pad\n",
    "\n",
    "        # He init for ReLU\n",
    "        fan_in = in_ch * kH * kW\n",
    "        self.W = (np.random.randn(out_ch, in_ch, kH, kW).astype(np.float32) * np.sqrt(2.0/fan_in))\n",
    "        self.b = np.zeros((out_ch,), dtype=np.float32)\n",
    "\n",
    "        # grads\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "        # cache\n",
    "        self.x_shape = None\n",
    "        self.cols = None\n",
    "        self.H_out = None\n",
    "        self.W_out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, C, H, W)\n",
    "        self.x_shape = x.shape\n",
    "        cols, H_out, W_out = im2col(x, self.kH, self.kW, self.stride, self.pad)\n",
    "        self.cols, self.H_out, self.W_out = cols, H_out, W_out\n",
    "\n",
    "        W_col = self.W.reshape(self.out_ch, -1)  # (out_ch, C*kH*kW)\n",
    "\n",
    "        out = cols @ W_col.T + self.b  # (N*H_out*W_out, out_ch)\n",
    "        out = out.reshape(x.shape[0], H_out, W_out, self.out_ch).transpose(0,3,1,2)  # (N,out_ch,H_out,W_out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # dout: (N, out_ch, H_out, W_out)\n",
    "        N = dout.shape[0]\n",
    "        dout_2d = dout.transpose(0,2,3,1).reshape(-1, self.out_ch)  # (N*H_out*W_out, out_ch)\n",
    "\n",
    "        # db\n",
    "        self.db = np.sum(dout_2d, axis=0)\n",
    "\n",
    "        # dW\n",
    "        self.dW = (dout_2d.T @ self.cols).reshape(self.W.shape)\n",
    "\n",
    "        # dX\n",
    "        W_col = self.W.reshape(self.out_ch, -1)\n",
    "        dcols = dout_2d @ W_col  # (N*H_out*W_out, C*kH*kW)\n",
    "        dx = col2im(dcols, self.x_shape, self.kH, self.kW, self.stride, self.pad, self.H_out, self.W_out)\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f0dbff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x > 0)\n",
    "        return x * self.mask\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d70d9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pool2D:\n",
    "    \"\"\"\n",
    "    Pooling layer supports:\n",
    "    - mode=\"max\"\n",
    "    - mode=\"avg\"\n",
    "    \"\"\"\n",
    "    def __init__(self, k=2, stride=2, mode=\"max\"):\n",
    "        self.k = k\n",
    "        self.stride = stride\n",
    "        self.mode = mode\n",
    "\n",
    "        self.x_shape = None\n",
    "        self.cols = None\n",
    "        self.H_out = None\n",
    "        self.W_out = None\n",
    "\n",
    "        # for max backward\n",
    "        self.argmax = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, C, H, W)\n",
    "        self.x_shape = x.shape\n",
    "        N, C, H, W = x.shape\n",
    "        kH = kW = self.k\n",
    "\n",
    "        cols, H_out, W_out = im2col(x, kH, kW, self.stride, pad=0)\n",
    "        # cols: (N*H_out*W_out, C*k*k)\n",
    "        self.cols, self.H_out, self.W_out = cols, H_out, W_out\n",
    "\n",
    "        cols = cols.reshape(N*H_out*W_out, C, kH*kW)\n",
    "\n",
    "        if self.mode == \"max\":\n",
    "            self.argmax = np.argmax(cols, axis=2)\n",
    "            out = np.max(cols, axis=2)\n",
    "        else:  # avg\n",
    "            self.argmax = None\n",
    "            out = np.mean(cols, axis=2)\n",
    "\n",
    "        out = out.reshape(N, H_out, W_out, C).transpose(0,3,1,2)  # (N,C,H_out,W_out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # dout: (N, C, H_out, W_out)\n",
    "        N, C, H_out, W_out = dout.shape\n",
    "        kH = kW = self.k\n",
    "\n",
    "        dout_ = dout.transpose(0,2,3,1).reshape(-1, C)  # (N*H_out*W_out, C)\n",
    "\n",
    "        dcols = np.zeros((N*H_out*W_out, C, kH*kW), dtype=np.float32)\n",
    "\n",
    "        if self.mode == \"max\":\n",
    "            idx = self.argmax  # (N*H_out*W_out, C)\n",
    "            for i in range(dcols.shape[0]):\n",
    "                dcols[i, np.arange(C), idx[i]] = dout_[i]\n",
    "        else:  # avg\n",
    "            dcols[:] = (dout_[:, :, None] / (kH*kW))\n",
    "\n",
    "        dcols = dcols.reshape(N*H_out*W_out, -1)  # (N*H_out*W_out, C*k*k)\n",
    "\n",
    "        dx = col2im(dcols, self.x_shape, kH, kW, self.stride, pad=0, H_out=self.H_out, W_out=self.W_out)\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56020d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        # He init\n",
    "        self.W = (np.random.randn(in_dim, out_dim).astype(np.float32) * np.sqrt(2.0/in_dim))\n",
    "        self.b = np.zeros((out_dim,), dtype=np.float32)\n",
    "\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, D)\n",
    "        self.x = x\n",
    "        return x @ self.W + self.b\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # dout: (N, out_dim)\n",
    "        self.dW = self.x.T @ dout\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        dx = dout @ self.W.T\n",
    "        return dx\n",
    "\n",
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.shape = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.shape = x.shape\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout.reshape(self.shape)\n",
    "\n",
    "def softmax_cross_entropy(logits, y):\n",
    "    \"\"\"\n",
    "    logits: (N, C)\n",
    "    y: (N,) integer labels\n",
    "    returns: loss (scalar), dlogits (N,C)\n",
    "    \"\"\"\n",
    "    N = logits.shape[0]\n",
    "    probs = softmax(logits)\n",
    "    loss = -np.mean(np.log(probs[np.arange(N), y] + 1e-12))\n",
    "    dlogits = probs.copy()\n",
    "    dlogits[np.arange(N), y] -= 1.0\n",
    "    dlogits /= N\n",
    "    return loss, dlogits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aab47c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    \"\"\"\n",
    "    CNN for MNIST:\n",
    "    Conv(1->8,3x3,pad=1) -> ReLU -> Pool(2)\n",
    "    Conv(8->16,3x3,pad=1) -> ReLU -> Pool(2)\n",
    "    Flatten -> FC(16*7*7 -> 10)\n",
    "    \"\"\"\n",
    "    def __init__(self, pool_mode=\"max\"):\n",
    "        self.c1 = Conv2D(1, 8, 3, 3, stride=1, pad=1)\n",
    "        self.a1 = ReLU()\n",
    "        self.p1 = Pool2D(k=2, stride=2, mode=pool_mode)\n",
    "\n",
    "        self.c2 = Conv2D(8, 16, 3, 3, stride=1, pad=1)\n",
    "        self.a2 = ReLU()\n",
    "        self.p2 = Pool2D(k=2, stride=2, mode=pool_mode)\n",
    "\n",
    "        self.f  = Flatten()\n",
    "        self.fc = Linear(16*7*7, 10)\n",
    "\n",
    "        self.layers = [self.c1, self.a1, self.p1, self.c2, self.a2, self.p2, self.f, self.fc]\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        return out  # logits\n",
    "\n",
    "    def backward(self, dlogits):\n",
    "        dout = dlogits\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def params_and_grads(self):\n",
    "        # Yield trainable parameters for optimizer\n",
    "        for layer in [self.c1, self.c2, self.fc]:\n",
    "            yield layer.W, layer.dW\n",
    "            yield layer.b, layer.db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f92dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop (mini-batch SGD)\n",
    "def batch_iter(X, y, batch_size=128, shuffle=True, seed=42):\n",
    "    idx = np.arange(len(X))\n",
    "    if shuffle:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        rng.shuffle(idx)\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        j = idx[i:i+batch_size]\n",
    "        yield X[j], y[j]\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, params_and_grads):\n",
    "        for p, g in params_and_grads:\n",
    "            p -= self.lr * g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23ebae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 0.5462 acc 0.8342 | test loss 0.2879 acc 0.9134\n",
      "Epoch 02 | train loss 0.2230 acc 0.9349 | test loss 0.1708 acc 0.9500\n",
      "Epoch 03 | train loss 0.1646 acc 0.9521 | test loss 0.1333 acc 0.9621\n"
     ]
    }
   ],
   "source": [
    "# Train + evaluate\n",
    "def evaluate(model, X, y, batch_size=256):\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for xb, yb in batch_iter(X, y, batch_size=batch_size, shuffle=False):\n",
    "        logits = model.forward(xb)\n",
    "        loss, _ = softmax_cross_entropy(logits, yb)\n",
    "        losses.append(loss)\n",
    "\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        correct += np.sum(preds == yb)\n",
    "        total += len(yb)\n",
    "    return float(np.mean(losses)), float(correct / total)\n",
    "\n",
    "def train_cnn(model, X_train, y_train, X_test, y_test, epochs=5, lr=0.02, batch_size=128):\n",
    "    opt = SGD(lr=lr)\n",
    "    history = {\"train_loss\":[], \"train_acc\":[], \"test_loss\":[], \"test_acc\":[]}\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # train\n",
    "        train_losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for xb, yb in batch_iter(X_train, y_train, batch_size=batch_size, shuffle=True, seed=42+ep):\n",
    "            logits = model.forward(xb)\n",
    "            loss, dlogits = softmax_cross_entropy(logits, yb)\n",
    "\n",
    "            model.backward(dlogits)\n",
    "            opt.step(model.params_and_grads())\n",
    "\n",
    "            train_losses.append(loss)\n",
    "            preds = np.argmax(logits, axis=1)\n",
    "            correct += np.sum(preds == yb)\n",
    "            total += len(yb)\n",
    "\n",
    "        tr_loss = float(np.mean(train_losses))\n",
    "        tr_acc = float(correct / total)\n",
    "\n",
    "        te_loss, te_acc = evaluate(model, X_test, y_test)\n",
    "\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"test_loss\"].append(te_loss)\n",
    "        history[\"test_acc\"].append(te_acc)\n",
    "\n",
    "        print(f\"Epoch {ep:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | test loss {te_loss:.4f} acc {te_acc:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "cnn = SimpleCNN(pool_mode=\"max\")\n",
    "hist = train_cnn(cnn, X_train, y_train, X_test, y_test, epochs=6, lr=0.02, batch_size=128)\n",
    "\n",
    "print(\"Final test acc:\", hist[\"test_acc\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79265de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist[\"train_loss\"], label=\"train\")\n",
    "plt.plot(hist[\"test_loss\"], label=\"test\")\n",
    "plt.title(\"CNN (NumPy) — Loss\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist[\"train_acc\"], label=\"train\")\n",
    "plt.plot(hist[\"test_acc\"], label=\"test\")\n",
    "plt.title(\"CNN (NumPy) — Accuracy\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"acc\"); plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce539c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Filter Visualization (8 pts)\n",
    "def show_conv_filters(W, title=\"Conv filters\", max_filters=8):\n",
    "    \"\"\"\n",
    "    W: (out_ch, in_ch, kH, kW)\n",
    "    For MNIST in_ch=1, so we plot out_ch filters as images.\n",
    "    \"\"\"\n",
    "    out_ch = W.shape[0]\n",
    "    n = min(out_ch, max_filters)\n",
    "    plt.figure(figsize=(2*n, 2))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(W[i, 0], cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"f{i}\")\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "show_conv_filters(cnn.c1.W, \"Layer1 Conv Filters (3x3)\")\n",
    "show_conv_filters(cnn.c2.W, \"Layer2 Conv Filters (3x3)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78bc4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_to_layer1_maps(model, x_one):\n",
    "    \"\"\"\n",
    "    x_one: (1,1,28,28)\n",
    "    returns feature maps after first conv+relu: (1,8,28,28)\n",
    "    \"\"\"\n",
    "    z = model.c1.forward(x_one)\n",
    "    a = model.a1.forward(z)\n",
    "    return a\n",
    "\n",
    "# pick one test image\n",
    "idx = 0\n",
    "x_one = X_test[idx:idx+1]\n",
    "y_one = y_test[idx]\n",
    "\n",
    "maps = forward_to_layer1_maps(cnn, x_one)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(x_one[0,0], cmap=\"gray\")\n",
    "plt.title(f\"Input image (label={y_one})\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# show first-layer feature maps\n",
    "n_show = maps.shape[1]\n",
    "plt.figure(figsize=(2*n_show, 2))\n",
    "for i in range(n_show):\n",
    "    plt.subplot(1, n_show, i+1)\n",
    "    plt.imshow(maps[0,i], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"map{i}\")\n",
    "plt.suptitle(\"Layer1 Feature Maps (Conv+ReLU)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9fb662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Receptive Field Study (4 pts)\n",
    "def receptive_field(layers_spec):\n",
    "    \"\"\"\n",
    "    Compute theoretical receptive field for a stack of layers.\n",
    "    layers_spec: list of dicts, e.g.\n",
    "      {\"type\":\"conv\", \"k\":3, \"s\":1}\n",
    "      {\"type\":\"pool\", \"k\":2, \"s\":2}\n",
    "\n",
    "    Returns (rf, jump) where:\n",
    "      rf   = receptive field size\n",
    "      jump = effective stride between adjacent features\n",
    "    \"\"\"\n",
    "    rf = 1\n",
    "    jump = 1\n",
    "    for layer in layers_spec:\n",
    "        k = layer[\"k\"]\n",
    "        s = layer[\"s\"]\n",
    "        rf = rf + (k - 1) * jump\n",
    "        jump = jump * s\n",
    "    return rf, jump\n",
    "\n",
    "spec = [\n",
    "    {\"type\":\"conv\", \"k\":3, \"s\":1},  # conv1\n",
    "    {\"type\":\"pool\", \"k\":2, \"s\":2},  # pool1\n",
    "    {\"type\":\"conv\", \"k\":3, \"s\":1},  # conv2\n",
    "    {\"type\":\"pool\", \"k\":2, \"s\":2},  # pool2\n",
    "]\n",
    "rf, jump = receptive_field(spec)\n",
    "print(\"Theoretical receptive field after pool2:\", rf, \"x\", rf, \"| jump:\", jump)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Pooling comparison (max vs avg) (3 pts)\n",
    "cnn_max = SimpleCNN(pool_mode=\"max\")\n",
    "hist_max = train_cnn(cnn_max, X_train, y_train, X_test, y_test, epochs=4, lr=0.02, batch_size=128)\n",
    "\n",
    "cnn_avg = SimpleCNN(pool_mode=\"avg\")\n",
    "hist_avg = train_cnn(cnn_avg, X_train, y_train, X_test, y_test, epochs=4, lr=0.02, batch_size=128)\n",
    "\n",
    "print(\"Final test acc (max):\", hist_max[\"test_acc\"][-1])\n",
    "print(\"Final test acc (avg):\", hist_avg[\"test_acc\"][-1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist_max[\"test_acc\"], label=\"maxpool\")\n",
    "plt.plot(hist_avg[\"test_acc\"], label=\"avgpool\")\n",
    "plt.title(\"Pooling Strategy Comparison — Test Accuracy\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"acc\"); plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fccb6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
